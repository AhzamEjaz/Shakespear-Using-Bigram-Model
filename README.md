# Shakespear-Using-Bigram-Model
**Description** : The model predicts next character in the stream of characters after the context length. \
**Tools** : Pytorch, Matplotlib, Jupyter Notebook

---
[Hyperparameters]\
**Total Batches used for training**  : 4,00,000 \
**Batch Size**  : 16 \
**Context length**  : 8 \
**Learning Rate**  : 0.001

---
[Tokenization method]\
Characters Tokenization.

---
[Underlying model]\
Used Embedding table for prediction and training.

---
[Results]\
**Loss**  : 0.081 \
**Accuracy**  : 68.13%
